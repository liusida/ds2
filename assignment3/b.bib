
@article{ha_world_2018,
	title = {World {Models}},
	url = {http://arxiv.org/abs/1803.10122},
	doi = {10.5281/zenodo.1207631},
	abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
	urldate = {2021-02-15},
	journal = {arXiv:1803.10122 [cs, stat]},
	author = {Ha, David and Schmidhuber, Jürgen},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.10122},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/liusida/Zotero/storage/3KXJFVU3/Ha and Schmidhuber - 2018 - World Models.pdf:application/pdf;arXiv.org Snapshot:/home/liusida/Zotero/storage/7IQBU8BI/1803.html:text/html}
}

@article{wang_survey_2020,
	title = {A {Survey} on {Bayesian} {Deep} {Learning}},
	volume = {53},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3409383},
	doi = {10.1145/3409383},
	abstract = {A comprehensive artificial intelligence system needs to not only perceive the environment with different “senses” (e.g., seeing and hearing) but also infer the world’s conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks, such as visual object recognition and speech recognition, using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models.1 In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and, in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, and so on. We also discuss the relationship and differences between Bayesian deep learning and other related topics, such as Bayesian treatment of neural networks.},
	number = {5},
	urldate = {2021-03-10},
	journal = {ACM Computing Surveys},
	author = {Wang, Hao and Yeung, Dit-Yan},
	month = sep,
	year = {2020},
	keywords = {Bayesian networks, Deep learning, generative models, probabilistic graphical models},
	pages = {108:1--108:37},
	file = {Full Text PDF:/home/liusida/Zotero/storage/4DI583HE/Wang and Yeung - 2020 - A Survey on Bayesian Deep Learning.pdf:application/pdf}
}

@article{real_regularized_2019,
	title = {Regularized {Evolution} for {Image} {Classifier} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1802.01548},
	abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9\% / 96.6\% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
	urldate = {2021-03-10},
	journal = {arXiv:1802.01548 [cs]},
	author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
	month = feb,
	year = {2019},
	note = {arXiv: 1802.01548},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, I.2.6, Computer Science - Distributed, Parallel, and Cluster Computing, I.5.1, I.5.2},
	annote = {Comment: Accepted for publication at AAAI 2019, the Thirty-Third AAAI Conference on Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/liusida/Zotero/storage/MR5XMYLP/Real et al. - 2019 - Regularized Evolution for Image Classifier Archite.pdf:application/pdf;arXiv.org Snapshot:/home/liusida/Zotero/storage/ALBBIDPS/1802.html:text/html}
}

@article{van_de_meent_introduction_2018,
	title = {An {Introduction} to {Probabilistic} {Programming}},
	url = {http://arxiv.org/abs/1809.10756},
	abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
	urldate = {2021-03-11},
	journal = {arXiv:1809.10756 [cs, stat]},
	author = {van de Meent, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.10756},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Statistics - Machine Learning},
	annote = {Comment: Under review at Foundations and Trends in Machine Learning},
	file = {arXiv Fulltext PDF:/home/liusida/Zotero/storage/5NF9FBHQ/van de Meent et al. - 2018 - An Introduction to Probabilistic Programming.pdf:application/pdf;arXiv.org Snapshot:/home/liusida/Zotero/storage/RRBCKMBS/1809.html:text/html}
}

@article{davidson_hyperspherical_2018,
	title = {Hyperspherical {Variational} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1804.00891},
	abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \${\textbackslash}mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-VAE, in low dimensions on other data types.},
	urldate = {2021-03-12},
	journal = {arXiv:1804.00891 [cs, stat]},
	author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
	month = sep,
	year = {2018},
	note = {arXiv: 1804.00891},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: GitHub repository: http://github.com/nicola-decao/s-vae-tf, Blogpost: https://nicola-decao.github.io/s-vae},
	file = {arXiv Fulltext PDF:/home/liusida/Zotero/storage/5Q8PY7IH/Davidson et al. - 2018 - Hyperspherical Variational Auto-Encoders.pdf:application/pdf;arXiv.org Snapshot:/home/liusida/Zotero/storage/JGXGMJX8/1804.html:text/html}
}

@article{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2021-03-12},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/liusida/Zotero/storage/2J3IF6KT/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/home/liusida/Zotero/storage/ZZALS2N8/1312.html:text/html}
}
